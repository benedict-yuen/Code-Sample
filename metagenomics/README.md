# Lucinid Symbiont Genome Recovery

## Overview 
This directory contains scripts used to recover and classify lucinid bacterial symbiont MAGs for various lucinid symbiosis projects.
These scripts were used to preprocess metagenome reads, assemble scaffolds, bin MAGs, classify bacterial symbionts, and carry out functional annotation  to study their biodiversity and metabolic functions.
The code was written in BASH for Illumina metagenome reads.


## Files and Functions

| Filename                                      | Description |
|-----------------------------------------------|------------|
| `01_filter_reads.sh`                          | Filters and trims raw sequencing reads. |
| `02_metagenomics_spades.sh`                   | Assembles metagenomic reads using SPAdes. |
| `03_metagenomics_binning.sh`                  | Performs metagenomic binning using three different binner to group contigs into genomes. |
| `04_metagenomics_gtdb_classification.sh`      | Classifies metagenomic bins using GTDB. |
| `05_metagenomics_checkm2.sh`                  | Assesses bin quality using CheckM2. |
| `06_metagenomics_DRAM_function_annotation.sh` | Annotates metabolic functions using DRAM. |
| `07_metagenomics_phylogenomics.sh`            | Conducts phylogenomic analysis of metagenomic bins. |


## Prerequisites

`01_filter_reads.sh`
- bbmap

`02_metagenomics_spades.sh`
- spades

`03_metagenomics_binning.sh`
- singularity
- anaconda
- bbmap (conda)
- BWA (module)
- samtools (HPC module)
- metabat2 (singularity container)
- binsanity (singularity container)
- maxbin2 (singularity container)
- dastool (singularity container)

`04_metagenomics_gtdb_classification.sh`
- singularity
- gtdbtk v2.1.1

`05_metagenomics_checkm2.sh`


`06_metagenomics_DRAM_function_annotation.sh`


`07_metagenomics_phylogenomics.sh`


## Running the Scripts

`sbatch 01_filter_reads.sh readFile1.gz readFile2.gz`

`sbatch 02_metagenomics_spades.sh <READS.FQ> <OUTDIR>`

`sbatch 03_metagenomics_binning.sh` 
This has to be run in the directory containing the assembly scaffolds with reads in ../*fastq.gz

`sbatch 04_metagenomics_gtdb_classification.sh`
Run this in the directory containing all the MAG fasta files to be classified

`sbatch 05_metagenomics_checkm2.sh`
Run this in the directory containing all the MAG fasta files to be QCed

`sbatch 06_metagenomics_DRAM_function_annotation.sh <GENOME.fasta>`

`sbatch 07_metagenomics_phylogenomics.sh <ALIGNMENT.fasta>`
Use the alignment generated by GTDB-TK workflow. It can be found in this location (gtdb_output/align/gtdbtk.bac120.user_msa.fasta.gz) generated by 04_metagenomics_gtdb_classification.sh.

## Notes & Potential Improvements
These scripts were designed for lucinid gill metagenome Illumina reads and for use on the GWDG HPC, and will need modifications for other data and HPCs
The scale of the projects utilizing these scripts was small and modifications are needed for scaled-up implementation. 

While the current metagenomics workflow functions as intended, I would enhance it in the future by implementing Snakemake to orchestrate each step of the process. Snakemake would allow for better workflow automation, ensuring that each analysis step is executed in sequence only if the previous step has completed successfully.

Additionally, I would improve the environment management by incorporating more consistent use of Conda environments or Singularity containers. This would ensure that all dependencies are clearly defined and reproducible, minimizing potential issues related to software compatibility across different systems.

Suggested Enhancements:
- Workflow orchestration with Snakemake: I would use Snakemake to define and execute each interdependent step to create a scalable pipeiline, track progress, and handle errors.
- More consistent use of Conda or Singularity: To ensure this workflow runs replicably seamlessly across different computing environments for different users, I would ensure that Conda environments and Singularity containers are implemented through the Snakemake pipeline.
- Modular and Scalable design: Using the Snakemake workflow would allow me to separate each step into distinct modules while still creating flexibility for adding new steps to optimise the workflow for different data types.

## Author & Contact
Written by Benedict Yuen as part of the Eco-Evolutionary Interactions group at the Max Planck Institute for Marine Microbiology. Reach me at byuen@mpi-bremen.de.
